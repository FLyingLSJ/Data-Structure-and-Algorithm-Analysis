如果 hypothesis 的个数是有限的，且样本数据足够大，那么机器学习是可行的。

# Recap and Preview

![img](images/5b55734452563.png)

上图是基于统计学的机器学习流程图

unknown P on X :表示所有数据样本来自于一定统计学规律的数据。训练样本 D 和最终测试 h 的样本都是来自同一个数据分布，这是机器能够学习的前提。

$E_{in}$ :表示在训练样本上错误的分数，$E_{out}$ 表示在训练样本以外错误的分数。机器学习做的事就是，在训练样本上面学习一个 hypothesis 使得训练样本上的误差尽可能的小，但是重要的是要在要能在训练集以外的样本上误差得到最小，这样机器学习才有意义。

机器学习的两个核心问题：

- $E_{in}(g)\approx E_{out}(g)$ 两者尽可能的相近

- $E_{in}(g)$ 足够的小

  ![img](images/5b557383da0fd.png)


![img](/images/5b557383da0fd-1545724352386.png)

在上面的证明说道：只要 M 较小， N 足够大，机器学习就是可行的。但是

- 若 M 较小，的确能符合第一个条件$E_{in}(g)\approx E_{out}(g)$，但是不能保证 $E_{in}(g)$ 足够的小，因为 hypothesis 的选择有限。
- 若 M 较大，就是反过来 hypothesis 有足够多的选择，能够确保$E_{in}(g)$ 足够的小，但是另外一个问题就不能保证。

